{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "\n",
    "## The Data\n",
    "\n",
    "### Handwritten Digit Recognition Dataset (MNIST)\n",
    "\n",
    "In this assignment we will be using the [MNIST digit dataset](https://yann.lecun.com/exdb/mnist/). \n",
    "\n",
    "The dataset contains images of hand-written digits ($0-9$), and the corresponding labels. \n",
    "\n",
    "The images have a resolution of $28\\times 28$ pixels.\n",
    "\n",
    "### The MNIST Dataset in TensorFlow\n",
    "\n",
    "You can use the tensorflow build-in functionality to download and import the dataset into python (see *Setup* section below).\n",
    "\n",
    "## The Assignment\n",
    "\n",
    "### Objectives\n",
    "\n",
    "You will use TensorFlow to implement several neural network models (labelled Model 1-4, and described in the corresponding sections of the Colab).\n",
    "\n",
    "You will then train these models to classify hand written digits from the Mnist dataset.\n",
    "\n",
    "### Variable Initialization\n",
    "\n",
    "Initialize the variables containing the parameters using [Xavier initialization](http://proceedings.mlr.press/v9/glorot10a.html).\n",
    "\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    "    my_variable = tf.Variable(initializer(shape))\n",
    "\n",
    "### Hyper-parameters\n",
    "\n",
    "For each of these models you will be requested to run experiments with different hyper-parameters.\n",
    "\n",
    "More specifically, you will be requested to try 3 sets of hyper-parameters per model, and report the resulting model accuracy.\n",
    "\n",
    "Each combination of hyper-parameter will specify how to set each of the following:\n",
    "\n",
    "- **num_epochs**: Number of iterations through the training section of the dataset [*a positive integer*].\n",
    "\n",
    "- **learning_rate**: Learning rate used by the gradient descent optimizer [*a scalar between 0 and 1*]\n",
    "\n",
    "In all experiments use a *batch_size* of 100.\n",
    "\n",
    "### Loss function\n",
    "All models, should be trained as to minimize the **cross-entropy loss** function:\n",
    "$$\n",
    "\\mathrm{loss}\n",
    "~~=~~\n",
    "-\\sum_{i=1}^N \\log p(y_i|x_i, \\theta)\n",
    "~~=~~\n",
    "-\\sum_{i=1}^N \\log{ \\underbrace{\\left(\\frac{\\exp(z_{i}[y_i])}{\\sum_{c=1}^{10} \\exp(z_{i}[c])}\\right)}_{\\text{softmax output}}}\n",
    "~~=~~\n",
    "\\sum_{i=1}^N \\left( -z_{i}[y_i] + \\log{\\left( \\sum_{c=1}^{10} \\exp(z_{i}[c]) \\right)} \\right)$$\n",
    "where $z \\in \\mathbb{R}^{10}$ is the input to the softmax layer and $z{[c]}$ denotes the $c$-th entry of vector $z$. And $i$ is a index for the dataset $\\{(x_i, y_i)\\}_{i=1}^N$.\n",
    "\n",
    "*Note*: Sum the loss across the elements of the batch with tf.reduce_sum().\n",
    "\n",
    "*Hint*: read about TensorFlow's [tf.nn.softmax_cross_entropy_with_logits](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits) function.\n",
    "\n",
    "### Optimization\n",
    "\n",
    "Use **stochastic gradient descent (SGD)** for optimizing the loss function.\n",
    "\n",
    "Hint: read about TensorFlow's [tf.train.GradientDescentOptimizer()](https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer).\n",
    "\n",
    "\n",
    "### Training and Evaluation\n",
    "\n",
    "The tensorflow built-in functionality for downloading and importing the dataset into python returns a Datasets object.\n",
    "\n",
    "This object will have three attributes: \n",
    "\n",
    "- train\n",
    "\n",
    "- validation\n",
    "\n",
    "- test\n",
    "\n",
    "Use only the **train** data in order to optimize the model.\n",
    "\n",
    "Use *datasets.train.next_batch(100)* in order to sample mini-batches of data.\n",
    "\n",
    "Every 20000 training samples (i.e. every 200 updates to the model), interrupt training and measure the accuracy of the model, \n",
    "\n",
    "each time evaluate the accuracy of the model both on the entire **test** set.\n",
    "\n",
    "### Reporting\n",
    "\n",
    "For each model i, you will collect the learning curves associated to each combination of hyper-parameters.\n",
    "\n",
    "Use the utility function `plot_learning_curves` to plot these learning curves,\n",
    "\n",
    "and the and utility function `plot_summary_table` to generate a summary table of results.\n",
    "\n",
    "For each run collect the train and test curves in a tuple, together with the hyper-parameters.\n",
    "\n",
    "    experiments_task_i = [\n",
    "\n",
    "       (num_epochs_1, learning_rate_1), train_accuracy_1, test_accuracy_1),\n",
    "    \n",
    "       (num_epochs_2, learning_rate_2), train_accuracy_2, test_accuracy_2),\n",
    "    \n",
    "       (num_epochs_3, learning_rate_3), train_accuracy_3, test_accuracy_3)]\n",
    "\n",
    "### Hint \n",
    "\n",
    "If you need some extra help, familiarizing yourselves with the dataset and the task of building models in TensorFlow, you can check the [TF tutorial for MNIST](https://www.tensorflow.org/tutorials/mnist/beginners/). \n",
    "\n",
    "The tutorial will walk you through the MNIST classification task step-by-step, building and optimizing a model in TensorFlow. \n",
    "\n",
    "(Please do not copy the provided code, though. Walk through the tutorial, but write your own implementation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and utility functions (do not modify!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import useful libraries.\n",
    "import numpy as np\n",
    "import pprint\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Global variables.\n",
    "log_period_samples = 20000\n",
    "batch_size = 100\n",
    "input_dim = 784\n",
    "output_dim = 10\n",
    "\n",
    "# Import dataset with one-hot encoding of the class labels.\n",
    "def get_data():\n",
    "  return input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Placeholders to feed train and test data into the graph.\n",
    "# Since batch dimension is 'None', we can reuse them both for train and eval.\n",
    "def get_placeholders():\n",
    "  x = tf.placeholder(tf.float32, [None, 784])\n",
    "  y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "  return x, y_\n",
    "\n",
    "# Plot learning curves of experiments\n",
    "def plot_learning_curves(experiment_data):\n",
    "  # Generate figure.\n",
    "  fig, axes = plt.subplots(3, 4, figsize=(22,12))\n",
    "  st = fig.suptitle(\n",
    "      \"Learning Curves for all Tasks and Hyper-parameter settings\",\n",
    "      fontsize=\"x-large\")\n",
    "  # Plot all learning curves.\n",
    "  for i, results in enumerate(experiment_data):\n",
    "    for j, setting in enumerate(results.keys()):\n",
    "      train_accuracy = results[setting]['train_accuracy']\n",
    "      test_accuracy = results[setting]['test_accuracy']\n",
    "      # Plot.\n",
    "      xs = [x * log_period_samples for x in range(1, len(train_accuracy)+1)]\n",
    "      axes[j, i].plot(xs, train_accuracy, label='train_accuracy')\n",
    "      axes[j, i].plot(xs, test_accuracy, label='test_accuracy')\n",
    "      # Prettify individual plots.\n",
    "      axes[j, i].ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "      axes[j, i].set_xlabel('Number of samples processed')\n",
    "      axes[j, i].set_ylabel('Epochs: {}, Learning rate: {}.  Accuracy'.format(*setting))\n",
    "      axes[j, i].set_title('Experiment {}'.format(i + 1))\n",
    "      axes[j, i].legend()\n",
    "  # Prettify overall figure.\n",
    "  plt.tight_layout()\n",
    "  st.set_y(0.95)\n",
    "  fig.subplots_adjust(top=0.91)\n",
    "  plt.show()\n",
    "\n",
    "# Generate summary table of results.\n",
    "def plot_summary_table(experiment_data):\n",
    "  # Fill Data.\n",
    "  cell_text = []\n",
    "  rows = []\n",
    "  columns = ['Setting 1', 'Setting 2', 'Setting 3']\n",
    "  for i, results in enumerate(experiment_data):\n",
    "    rows.append('Model {}'.format(i + 1))\n",
    "    cell_text.append([])\n",
    "    for j, setting in enumerate(results.keys()):\n",
    "      train_accuracy = results[setting]['train_accuracy']\n",
    "      test_accuracy = results[setting]['test_accuracy']\n",
    "      cell_text[i].append(test_accuracy[-1])\n",
    "  # Generate Table.\n",
    "  fig=plt.figure(frameon=False)\n",
    "  ax = plt.gca()\n",
    "  the_table = ax.table(\n",
    "      cellText=cell_text,\n",
    "      rowLabels=rows,\n",
    "      colLabels=columns,\n",
    "      loc='center')\n",
    "  the_table.scale(1, 4)\n",
    "  # Prettify.\n",
    "  ax.patch.set_facecolor('None')\n",
    "  ax.xaxis.set_visible(False)\n",
    "  ax.yaxis.set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 (20 pts)\n",
    "\n",
    "### Model\n",
    "\n",
    "Train a neural network model consisting of 1 linear layer, followed by a softmax:\n",
    "\n",
    "(input $\\rightarrow$ linear layer $\\rightarrow$ softmax $\\rightarrow$ class probabilities)\n",
    "\n",
    "### Hyper-parameters\n",
    "\n",
    "Train the model with three different hyper-parameter settings:\n",
    "\n",
    "- *num_epochs*=5, *learning_rate*=0.0001\n",
    "\n",
    "- *num_epochs*=5, *learning_rate*=0.005\n",
    "\n",
    "- *num_epochs*=5, *learning_rate*=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 (20 pts)\n",
    "\n",
    "1 hidden layer (32 units) with a ReLU non-linearity, followed by a softmax.\n",
    "\n",
    "(input $\\rightarrow$ non-linear layer $\\rightarrow$ linear layer $\\rightarrow$ softmax $\\rightarrow$ class probabilities)\n",
    "\n",
    "### Hyper-parameters\n",
    "\n",
    "Train the model with three different hyper-parameter settings:\n",
    "\n",
    "- *num_epochs*=15, *learning_rate*=0.0001\n",
    "\n",
    "- *num_epochs*=15, *learning_rate*=0.005\n",
    "\n",
    "- *num_epochs*=15, *learning_rate*=0.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3 (20 pts)\n",
    "\n",
    "2 hidden layers (32 units) each, with ReLU non-linearity, followed by a softmax.\n",
    "\n",
    "(input $\\rightarrow$ non-linear layer $\\rightarrow$ non-linear layer $\\rightarrow$ linear layer $\\rightarrow$ softmax $\\rightarrow$ class probabilities)\n",
    "\n",
    "\n",
    "### Hyper-parameters\n",
    "\n",
    "Train the model with three different hyper-parameter settings:\n",
    "\n",
    "- *num_epochs*=5, *learning_rate*=0.003\n",
    "\n",
    "- *num_epochs*=40, *learning_rate*=0.003\n",
    "\n",
    "- *num_epochs*=40, *learning_rate*=0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4 (20 pts)\n",
    "\n",
    "### Model\n",
    "3 layer convolutional model (2 convolutional layers followed by max pooling) + 1 non-linear layer (32 units), followed by softmax. \n",
    "\n",
    "(input(28x28) $\\rightarrow$ conv(3x3x8) + maxpool(2x2) $\\rightarrow$ conv(3x3x8) + maxpool(2x2) $\\rightarrow$ flatten $\\rightarrow$ non-linear $\\rightarrow$ linear layer $\\rightarrow$ softmax $\\rightarrow$ class probabilities)\n",
    "\n",
    "\n",
    "- Use *padding = 'SAME'* for both the convolution and the max pooling layers. \n",
    "\n",
    "- Employ plain convolution (no stride) and for max pooling operations use 2x2 sliding windows, with no overlapping pixels (note: this operation will down-sample the input image by 2x2).\n",
    "\n",
    "### Hyper-parameters\n",
    "\n",
    "Train the model with three different hyper-parameter settings:\n",
    "\n",
    "- num_epochs=5, learning_rate=0.01\n",
    "\n",
    "- num_epochs=10, learning_rate=0.001\n",
    "\n",
    "- num_epochs=20, learning_rate=0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 (5 pts): Indicate which of the previous experiments constitute an example of over-fitting. Why is this happening?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 (5 pts): Indicate which of the previous experiments constitute an example of under-fitting. Why is this happening?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 (10 pts): How would you prevent over-/under-fitting from happening?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_learning_curves([experiments_task1, experiments_task2, experiments_task3, experiments_task4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final test accuracies\n",
    "plot_summary_table([experiments_task1, experiments_task2, experiments_task3, experiments_task4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
